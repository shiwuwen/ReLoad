我们的算法Reload与A2C算法类似，该算法通过深度神经网络来预测一个策略π，该策略表征了对于给定的状态st应当采取的动作at。由于动作空间a的取值是一系列连续值，基于离散的方法对a进行划分会导致维度爆炸（该方法输出a的所有可能取值的排列组合，并选择概率最大的一个作为预测结果），例如，以0.1的粒度对a进行划分，当边缘服务器的个数为5时，可能的取值个数为1001，而当边缘服务器的个数为10时，取值个数将扩大到92378。为了解决该问题，我们将a的维度固定为n+1（其中n为边缘服务器个数），并将策略π作为softmax函数的输入，从而直接输出卸载到边缘服务器的工作比例。算法输出的结果如下所示：
 
此外，为了对environment进行探索，我们还引入了超参数clip_bound，该参数用于对上述算法得到的动作空间进行裁剪，从而获得更好的分配策略。在我们的实验中，clip_bound设置为0.9能够取得较好的效果。
与A2C算法一样，我们使用梯度下降策略来训练我们的算法，其梯度公式如下所示：
 
其中A(s,a)被称为优势函数，其具体形式如下所示：
 
其中Q(s,a)为state-action value function，V(s)为state value function。但在实际的实现过程中，我们并不需要同时预测两个函数，仅使用V(s)便能达到同样的效果。我们基于Temporal Differnece method对A(s,a)进行估计，得到简化后的梯度公式为：
 
其中 是对A(s,a)的无偏估计，被称为td_error，由critic网络生成。基于上述公式，actor网络的参数更新方式如下：
 
其中α为learning rate，我们将其固定为0.001。γ为reward discount，我们将其固定为0.9。r为时刻t environment返回的reward。
为了使actor网络得到的结果更加准确，我们使用critic网络来预测td_error。在critic网络中，基于平方损失函数的参数更新方式如下所示：
 
其中α,λ均与actor网络中相同。
在我们的实验中，对于不同的边缘服务器数量n，使得上述参数收敛所需的迭代次数是不同的。当n=10时，我们设置episode=300，并得到了稳定的结果，而当n=20时，仅需要100个迭代周期便能达到同样的效果。尽管episode存在变化，但每个episode包含的step并不需要做相应的调整，我们将其固定为100，在我们的实验中这是最优的选择。

